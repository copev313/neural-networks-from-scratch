{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Activation Functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We use different activation functions for different cases, so understanding how they work can help us properly pick which of them is best for a task.\n",
    "\n",
    "> The **activation function** is applied to the output of a neuron (or layer of neurons), which modifies outputs.\n",
    "\n",
    "> If an activation function itself is nonlinear, it allows for neural networks with multiple hidden layers to map nonlinear functions.\n",
    "\n",
    "> In general, neural networks will have two types of activation functions. Those used in hidden layers and those used in the output layer. Usually, the activation function used for the hidden neurons will be the same for all of them, but this doesn't always have to be the case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "### The Step Activation Function\n",
    "\n",
    "> Recall the purpose this activation function serves is to mimic a neuron's \"firing\" or \"not firing\" based on input information. The simplest version of this is the **step function**. In a single neuron, if the `weights * inputs + bias` is greater than 0, the neuron fires, otherwise it does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "    - Historically used in hidden layers.\n",
    "    - Rarely chosen today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/step_function.png\" width=\"550\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### The Linear Activation Function\n",
    "\n",
    "> A **linear function** is simply the equation of a line. It is defined as `f(x) = x` or `y=x`.\n",
    "\n",
    "Summary:\n",
    "\n",
    "    - Usually applied to the last layer's output in a regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recall a regression model outputs a scalar value instead of a classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/linear_function.webp\" width=\"550\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "### The Sigmoid Activation Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When it comes to optimizing weights and biases, it's often easier for the optimizer when we have activation functions that are more granular and informative. The original, more granular, activation function used for neural networks was the **Sigmoid** activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sigmoid_function.jpg\" width=\"550\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The function returns a value in the range of 0 for negative infinity, through 0.5 for the input of zero, and to 1 for positive infinity.\n",
    "\n",
    "> The output of the Sigmoid function, being in the rangeof 0 to 1, make for better use in neural networks. Especially compared to the range of the negative to positive infinity. This adds nonlinearity to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "    - Historically used in hidden layers.\n",
    "    - Eventually replaced by the **Rectified Linear Unit (ReLU)** activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "### The Rectfied Linear Activation Function\n",
    "\n",
    "> The rectified linear activation function is simpler than the sigmoid function. It is simply `y=x`, clipped at zero from the negative side. So if x is less than zero, y is zero, otherwise y is equal to x.\n",
    "\n",
    "> This function can also be calculated as `max(0, x)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/relu_function.png\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The ReLU function is the most commonly used activation function in neural networks today. This is mainly due to speed and efficiency.\n",
    "\n",
    "> The ReLU activation function is quite close to being linear, however it remains nonlinear. This is due to the bend after zero, a simple yet very effective property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "### Why Use Activation Functions?\n",
    "\n",
    "> In most cases, in order for a neural network to fit a nonlinear function, it must contain two or more hidden layers which use a nonlinear activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### ReLU Activation Function Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n",
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n",
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "# Method 1: Using cases --\n",
    "output = []\n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        output.append(i)\n",
    "    else:\n",
    "        output.append(0)\n",
    "\n",
    "print(output)\n",
    "\n",
    "# Method 2: Using max function --\n",
    "output = []\n",
    "for i in inputs:\n",
    "    output.append(max(0, i))\n",
    "\n",
    "print(output)\n",
    "\n",
    "# Method 3: Using numpy maximum function --\n",
    "import numpy as np\n",
    "\n",
    "output = np.maximum(0, inputs)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectified linear activation class:\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.        ]\n",
      " [0.         0.00011395 0.        ]\n",
      " [0.         0.00031729 0.        ]\n",
      " [0.         0.00052666 0.        ]\n",
      " [0.         0.00071401 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Apply this activation function to the dense layer's output in our code:\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from code.layers import Layer_Dense\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Create dataset with spiral data:\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values:\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Make a forward pass of our training data through this layer:\n",
    "dense1.forward(X)\n",
    "\n",
    "# Forward pass through activation function (takes in output from previous layer):\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation1.output[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
